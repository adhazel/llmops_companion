{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03 Part B Notebook\n",
    "\n",
    "Use this notebook to familiarize yourself with the flex flow syntax. Note the following references. \n",
    "\n",
    "- CLI reference: https://microsoft.github.io/promptflow/reference/pf-command-reference.html\n",
    "- SDK on getting started with flex flow: https://microsoft.github.io/promptflow/tutorials/flex-flow-quickstart.html\n",
    "- SDK on managing runs: https://microsoft.github.io/promptflow/how-to-guides/run-and-evaluate-a-flow/manage-runs.html\n",
    "- SDK on how to develop evaluation flows: https://microsoft.github.io/promptflow/how-to-guides/develop-a-dag-flow/develop-evaluation-flow.html\n",
    "- SDK how to on running and evaluating flows: https://microsoft.github.io/promptflow/how-to-guides/run-and-evaluate-a-flow/index.html\n",
    "- SDK on Getting started with prompty: https://microsoft.github.io/promptflow/tutorials/prompty-quickstart.html\n",
    "\n",
    "- Microsoft Learn on how evaluation flows: https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-develop-an-evaluation-flow?view=azureml-api-2#metrics-logging-and-aggregation-node\n",
    "- Microsoft Learn on custom evaluations: https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-develop-an-evaluation-flow?view=azureml-api-2\n",
    "- Example Flex Flow Chat with Functions: https://github.com/microsoft/promptflow/blob/main/examples/flex-flows/chat-with-functions/README.md\n",
    "- Example Flex Flow EvalFlow class example: https://github.com/microsoft/promptflow/blob/main/examples/flex-flows/eval-checklist/check_list.py\n",
    "- Open Source GitHub Prompt flow Examples on prompty: https://github.com/microsoft/promptflow/blob/2f6eccf46341d3b4141c0a35bac009130e29e08e/examples/prompty/basic/prompty-quickstart.ipynb\n",
    "- Open Source GitHub Prompt flow Tutorial on Quality Improvement: https://github.com/microsoft/promptflow/blob/main/examples/tutorials/flow-fine-tuning-evaluation/promptflow-quality-improvement.md\n",
    "- Open Source GitHub Prompt flow Tutorial on Tracing (very useful for flex flows + AutoGen): https://github.com/microsoft/promptflow/tree/main/examples/tutorials/tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File reference\n",
    "\n",
    "| Category| Path(s) | Description |\n",
    "| --- | --- | --- |\n",
    "| prompty | chat_with_tools.prompty | The generative AI prompt used for the main chat function. |\n",
    "| prompty | eval.prompty | The generative AI prompt used for the evaluation function. |\n",
    "| prompty | restate_question.prompty | The generative AI prompt used to restate the question. |\n",
    "| prompty | tools.json | The set of tools that are passed into the generative AI call through prompty. |\n",
    "| testing | data.jsonl | Data file used to test the chat function and evaluate the responses. |\n",
    "| entry | flow.flex.yaml | When prompty is used, yaml is not strictly required; however, for automated deployment, this may be required and is recommended. |\n",
    "| entry | my_flow_entry.py | The main python function. |\n",
    "| testing | sample.json | A sample chat history using this flow. |\n",
    "| other | README.md | Beginning instructions for this lab. |\n",
    "| other | requirements.txt | Library dependencies. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "    # load environment variables from .env file\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aprilhazel\\Source\\promptflow_codefirst_llmops\\labs\\lab03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aprilhazel\\\\Source\\\\promptflow_codefirst_llmops\\\\labs\\\\lab03'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify the second line as needed to get to the root of your prompt flow directory\n",
    "%cd \"C:\\Users\\aprilhazel/Source/promptflow_codefirst_llmops/labs/lab03\"\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_ai_connection (AzureOpenAI)\n"
     ]
    }
   ],
   "source": [
    "from promptflow.client import PFClient\n",
    "\n",
    "# initialize the client\n",
    "pf = PFClient()\n",
    "\n",
    "# List all the available connections\n",
    "for c in pf.connections.list():\n",
    "    print(c.name + \" (\" + c.type + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing connection\n",
      "auth_mode: key\n",
      "name: open_ai_connection\n",
      "module: promptflow.connections\n",
      "created_date: '2024-06-18T09:13:26.519864'\n",
      "last_modified_date: '2024-06-18T15:28:38.626652'\n",
      "type: azure_open_ai\n",
      "api_key: '******'\n",
      "api_base: https://coe-openai-sweden.openai.azure.com/\n",
      "api_type: azure\n",
      "api_version: '2024-02-01'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create needed connection\n",
    "from promptflow.entities import AzureOpenAIConnection, OpenAIConnection\n",
    "\n",
    "my_conn_name = \"open_ai_connection\"\n",
    "my_api_base=\"https://coe-openai-sweden.openai.azure.com/\"\n",
    "my_api_version=\"2024-02-01\"\n",
    "\n",
    "try:\n",
    "    result = pf.connections.get(name=my_conn_name)\n",
    "    print(\"using existing connection\")\n",
    "except:\n",
    "    # Follow https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal to create an Azure Open AI resource.\n",
    "    connection = AzureOpenAIConnection(\n",
    "        name=my_conn_name,\n",
    "        api_key=\"<user-input>\",\n",
    "        api_base=my_api_base,\n",
    "        api_type=\"azure\",\n",
    "        api_version=my_api_version,\n",
    "    )\n",
    "\n",
    "    # use this if you have an existing OpenAI account\n",
    "    # connection = OpenAIConnection(\n",
    "    #     name=conn_name,\n",
    "    #     api_key=\"<user-input>\",\n",
    "    # )\n",
    "    result = pf.connections.create_or_update(connection)\n",
    "    print(\"successfully created connection\")\n",
    "\n",
    "# print the connection details\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read prompty file into an object.\n",
    "\n",
    "Consider the \"tools\" listed and review the tools.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "name: Chat with Tools\n",
      "description: A basic prompt with tools\n",
      "model:\n",
      "    api: chat\n",
      "    configuration:\n",
      "      connection: open_ai_connection\n",
      "      type: azure_openai\n",
      "      azure_deployment: ${env:CHAT_MODEL_DEPLOYMENT_NAME}\n",
      "    parameters:\n",
      "      max_tokens: 128\n",
      "      temperature: 0.2\n",
      "      tools: ${file:tools.json}\n",
      "      tool_choice: auto\n",
      "inputs:\n",
      "  question:\n",
      "    type: string\n",
      "  chat_history:\n",
      "    type: list\n",
      "    default: []\n",
      "sample:\n",
      "  question: What's the weather in Beijing?\n",
      "  chat_history: []\n",
      "---\n",
      "# system:\n",
      "Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\n",
      "\n",
      "{% for item in chat_history %}\n",
      "{{item.role}}:\n",
      "{{item.content}}\n",
      "{% endfor %}\n",
      "\n",
      "user:\n",
      "{{question}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read prompty file\n",
    "with open(\"chat_with_tools.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the flow via prompty as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': None,\n",
       " 'role': 'assistant',\n",
       " 'function_call': None,\n",
       " 'tool_calls': [{'id': 'call_qGyxXkz2vpQwixgskDgHNwo5',\n",
       "   'function': {'arguments': '{\"location\":\"Paris, France\",\"format\":\"celsius\"}',\n",
       "    'name': 'get_current_weather'},\n",
       "   'type': 'function'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow.core import Prompty\n",
    "\n",
    "# load prompty as a flow\n",
    "f = Prompty.load(source=\"chat_with_tools.prompty\")\n",
    "\n",
    "# execute the flow as function\n",
    "response_message={}\n",
    "response_message = f(question=\"What is the weather in France?\")\n",
    "response_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add programatic know-how. For example, the generative AI flow returns the above tool_calls which are then executed dynamically via the function located here: my_flow_entry.py > chat > run_function. The below shows an example of how this is accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import my_flow_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing my_flow_entry module > function: get_current_weather with Arguments: {'location': 'Paris, France', 'format': 'celsius'}. Result is...\n",
      "{'location': 'Paris, France', 'temperature': '72', 'format': 'celsius', 'forecast': ['sunny', 'windy']}\n"
     ]
    }
   ],
   "source": [
    "# similar logic to what is used in the my_fow_entry.py file\n",
    "if \"tool_calls\" in response_message and len(response_message[\"tool_calls\"]) == 1:\n",
    "    call = response_message[\"tool_calls\"][0]\n",
    "    function = call[\"function\"]\n",
    "    function_name = function[\"name\"]\n",
    "    function_args = json.loads(function[\"arguments\"])\n",
    "    print(f\"Executing my_flow_entry module > function: {function_name} with Arguments: {function_args}. Result is...\")\n",
    "    function_to_call = getattr(my_flow_entry, function_name)\n",
    "    result = function_to_call(**function_args) \n",
    "    print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a trace session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt flow service...\n",
      "You can stop the prompt flow service with the following command:'\u001b[1mpf service stop\u001b[0m'.\n",
      "Alternatively, if no requests are made within 1 hours, it will automatically stop.\n"
     ]
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit a prompt flow run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the flow.flex.yaml flow using the default inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=lab03&uiTraceId=0x34f39e57920750f5a5219b8d2c134c92\n",
      "2024-06-19 22:55:13 -0500   22048 execution.flow     INFO     [Flex in line 0 (index starts from 0)] stdout> {'location': 'Beijing', 'format': 'celsius'}\n",
      "{'location': 'Beijing', 'temperature': '72', 'format': 'celsius', 'forecast': ['sunny', 'windy']}\n"
     ]
    }
   ],
   "source": [
    "!pf flow test --flow ./myflow.flex.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the flow.flex.yaml flow using inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=lab03&uiTraceId=0xf44cd788f3d35ad14ffe187800588e7d\n",
      "2024-06-19 23:00:51 -0500   28100 execution.flow     INFO     [Flex in line 0 (index starts from 0)] stdout> {'location': 'Paris', 'format': 'celsius'}\n",
      "{'location': 'Paris', 'temperature': '72', 'format': 'celsius', 'forecast': ['sunny', 'windy']}\n"
     ]
    }
   ],
   "source": [
    "!pf flow test --flow ./myflow.flex.yaml --inputs question=\"What is the weather in Paris?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the flow as function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_flow_entry import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location': 'Seattle, WA', 'format': 'fahrenheit'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'location': 'Seattle, WA', 'temperature': '72', 'format': 'fahrenheit', 'forecast': ['sunny', 'windy']}\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the flow as function\n",
    "result = chat(question=\"What's the weather of Seattle? I love that place!\")\n",
    "# note the type is generator object as we enabled stream in prompty\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit a batch run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=lab03_20240619_230508_028738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-19 23:05:12 -0500][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run lab03_20240619_230508_028738, log path: C:\\Users\\aprilhazel\\.promptflow\\.runs\\lab03_20240619_230508_028738\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-19 23:05:12 -0500   22100 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-06-19 23:05:12 -0500   22100 execution.bulk     INFO     Current system's available memory is 8332.734375MB, memory consumption of current process is 206.37890625MB, estimated available worker count is 8332.734375/206.37890625 = 40\n",
      "2024-06-19 23:05:12 -0500   22100 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 40}.\n",
      "2024-06-19 23:05:17 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-10)-Process id(30464)-Line number(0) start execution.\n",
      "2024-06-19 23:05:17 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-12)-Process id(19316)-Line number(1) start execution.\n",
      "2024-06-19 23:05:17 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-11)-Process id(30732)-Line number(2) start execution.\n",
      "2024-06-19 23:05:19 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-10)-Process id(30464)-Line number(0) completed.\n",
      "2024-06-19 23:05:20 -0500   22100 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-06-19 23:05:20 -0500   22100 execution.bulk     INFO     Average execution time for completed lines: 3.01 seconds. Estimated time for incomplete lines: 6.02 seconds.\n",
      "2024-06-19 23:05:22 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-11)-Process id(30732)-Line number(2) completed.\n",
      "2024-06-19 23:05:23 -0500   22100 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2024-06-19 23:05:23 -0500   22100 execution.bulk     INFO     Average execution time for completed lines: 3.01 seconds. Estimated time for incomplete lines: 3.01 seconds.\n",
      "2024-06-19 23:05:23 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-12)-Process id(19316)-Line number(1) completed.\n",
      "2024-06-19 23:05:24 -0500   22100 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-06-19 23:05:24 -0500   22100 execution.bulk     INFO     Average execution time for completed lines: 2.34 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-06-19 23:05:24 -0500   22100 execution.bulk     INFO     The thread monitoring the process [30464-SpawnProcess-10] will be terminated.\n",
      "2024-06-19 23:05:24 -0500   22100 execution.bulk     INFO     The thread monitoring the process [19316-SpawnProcess-12] will be terminated.\n",
      "2024-06-19 23:05:24 -0500   22100 execution.bulk     INFO     The thread monitoring the process [30732-SpawnProcess-11] will be terminated.\n",
      "2024-06-19 23:05:24 -0500   30464 execution.bulk     INFO     The process [30464] has received a terminate signal.\n",
      "2024-06-19 23:05:24 -0500   30732 execution.bulk     INFO     The process [30732] has received a terminate signal.\n",
      "2024-06-19 23:05:24 -0500   19316 execution.bulk     INFO     The process [19316] has received a terminate signal.\n",
      "2024-06-19 23:05:24 -0500   22100 execution.bulk     INFO     Process 30464 terminated.\n",
      "2024-06-19 23:05:24 -0500   22100 execution.bulk     INFO     Process 30732 terminated.\n",
      "2024-06-19 23:05:24 -0500   22100 execution.bulk     INFO     Process 19316 terminated.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"lab03_20240619_230508_028738\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-06-19 23:05:08.028738\"\n",
      "Duration: \"0:00:16.992137\"\n",
      "Output path: \"C:\\Users\\aprilhazel\\.promptflow\\.runs\\lab03_20240619_230508_028738\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from my_flow_entry import chat \n",
    "data = \"./data.jsonl\"  # path to the data file\n",
    "\n",
    "batch_run = pf.run(\n",
    "    flow=chat,\n",
    "    data=data,\n",
    "    stream=True,\n",
    "    column_mapping={\"question\": \"${data.question}\", \"chat_history\": \"${data.chat_history}\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.chat_history</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>inputs.max_total_token</th>\n",
       "      <th>outputs.output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How about London next week?</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>{'location': 'London', 'temperature': '60', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What will the weather be in St Louis, Missouri...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>{'location': 'St Louis, MO', 'temperature': '6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the capital of Japan?</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>The capital of Japan is Tokyo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     inputs.question inputs.chat_history  \\\n",
       "0                        How about London next week?                  []   \n",
       "1  What will the weather be in St Louis, Missouri...                  []   \n",
       "2                      What is the capital of Japan?                  []   \n",
       "\n",
       "   inputs.line_number  inputs.max_total_token  \\\n",
       "0                   0                    2048   \n",
       "1                   1                    2048   \n",
       "2                   2                    2048   \n",
       "\n",
       "                                      outputs.output  \n",
       "0  {'location': 'London', 'temperature': '60', 'f...  \n",
       "1  {'location': 'St Louis, MO', 'temperature': '6...  \n",
       "2                     The capital of Japan is Tokyo.  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the inputs/outputs details of a finished run.\n",
    "details = pf.get_details(batch_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add batch evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-19 23:08:48 -0500][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run lab03_20240619_230848_620512, log path: C:\\Users\\aprilhazel\\.promptflow\\.runs\\lab03_20240619_230848_620512\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=lab03_20240619_230848_620512\n",
      "2024-06-19 23:08:48 -0500   22100 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-06-19 23:08:48 -0500   22100 execution.bulk     INFO     Current system's available memory is 8391.48046875MB, memory consumption of current process is 208.5MB, estimated available worker count is 8391.48046875/208.5 = 40\n",
      "2024-06-19 23:08:48 -0500   22100 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 40}.\n",
      "2024-06-19 23:08:53 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-14)-Process id(18828)-Line number(0) start execution.\n",
      "2024-06-19 23:08:53 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-16)-Process id(25176)-Line number(1) start execution.\n",
      "2024-06-19 23:08:53 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-15)-Process id(14280)-Line number(2) start execution.\n",
      "2024-06-19 23:08:56 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-14)-Process id(18828)-Line number(0) completed.\n",
      "2024-06-19 23:08:56 -0500   22100 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-06-19 23:08:56 -0500   22100 execution.bulk     INFO     Average execution time for completed lines: 3.03 seconds. Estimated time for incomplete lines: 6.06 seconds.\n",
      "2024-06-19 23:08:59 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-15)-Process id(14280)-Line number(2) completed.\n",
      "2024-06-19 23:08:59 -0500   22100 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2024-06-19 23:08:59 -0500   22100 execution.bulk     INFO     Average execution time for completed lines: 3.03 seconds. Estimated time for incomplete lines: 3.03 seconds.\n",
      "2024-06-19 23:09:00 -0500   22100 execution.bulk     INFO     Process name(SpawnProcess-16)-Process id(25176)-Line number(1) completed.\n",
      "2024-06-19 23:09:00 -0500   22100 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-06-19 23:09:00 -0500   22100 execution.bulk     INFO     Average execution time for completed lines: 2.35 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-06-19 23:09:00 -0500   22100 execution.bulk     INFO     The thread monitoring the process [25176-SpawnProcess-16] will be terminated.\n",
      "2024-06-19 23:09:00 -0500   22100 execution.bulk     INFO     The thread monitoring the process [18828-SpawnProcess-14] will be terminated.\n",
      "2024-06-19 23:09:00 -0500   22100 execution.bulk     INFO     The thread monitoring the process [14280-SpawnProcess-15] will be terminated.\n",
      "2024-06-19 23:09:00 -0500   25176 execution.bulk     INFO     The process [25176] has received a terminate signal.\n",
      "2024-06-19 23:09:00 -0500   18828 execution.bulk     INFO     The process [18828] has received a terminate signal.\n",
      "2024-06-19 23:09:00 -0500   14280 execution.bulk     INFO     The process [14280] has received a terminate signal.\n",
      "2024-06-19 23:09:01 -0500   22100 execution.bulk     INFO     Process 18828 terminated.\n",
      "2024-06-19 23:09:01 -0500   22100 execution.bulk     INFO     Process 14280 terminated.\n",
      "2024-06-19 23:09:01 -0500   22100 execution.bulk     INFO     Process 25176 terminated.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"lab03_20240619_230848_620512\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-06-19 23:08:48.620512\"\n",
      "Duration: \"0:00:12.941114\"\n",
      "Output path: \"C:\\Users\\aprilhazel\\.promptflow\\.runs\\lab03_20240619_230848_620512\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<promptflow._sdk.entities._run.Run at 0x1d189698750>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# set eval flow path\n",
    "eval_flow = \"./eval.prompty\"\n",
    "data= \"./data.jsonl\"\n",
    "\n",
    "# run the flow with existing run\n",
    "eval_run = pf.run(\n",
    "    flow=eval_flow,\n",
    "    data=data,\n",
    "    run=batch_run,\n",
    "    column_mapping={  # map the url field from the data to the url input of the flow\n",
    "      \"chat_history\": \"${data.chat_history}\",\n",
    "      \"question\": \"${data.question}\",\n",
    "      \"answer\": \"${run.outputs.output}\", \n",
    "      \"ground_truth\": \"${data.ground_truth}\"\n",
    "      }\n",
    ")\n",
    "\n",
    "# stream the run until it's finished\n",
    "pf.stream(eval_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get details (Tip: Use tabulate to see the long text fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-----------------------+----------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    | inputs.question                                          | inputs.answer                                                                                                   | inputs.ground_truth                                                                                | inputs.chat_history   |   inputs.line_number |   outputs.score | outputs.explanation                                                                                                              |\n",
      "+====+==========================================================+=================================================================================================================+====================================================================================================+=======================+======================+=================+==================================================================================================================================+\n",
      "|  0 | How about London next week?                              | {'location': 'London', 'temperature': '60', 'format': 'celsius', 'forecast': ['rainy'], 'num_days': 7}          | {'location': 'London', 'temperature': '72', 'format': 'celsius', 'forecast': ['sunny', 'windy']}   | []                    |                    0 |               2 | The answer provided the correct location and format but incorrect temperature and forecast details compared to the ground truth. |\n",
      "+----+----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-----------------------+----------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  1 | What will the weather be in St Louis, Missouri tomorrow? | {'location': 'St Louis, MO', 'temperature': '60', 'format': 'fahrenheit', 'forecast': ['rainy'], 'num_days': 2} | {'location': 'St Louis', 'temperature': '72', 'format': 'celsius', 'forecast': ['sunny', 'windy']} | []                    |                    1 |               1 | The provided answer has incorrect temperature, temperature format, and forecast for the weather in St Louis, Missouri.           |\n",
      "+----+----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-----------------------+----------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  2 | What is the capital of Japan?                            | The capital of Japan is Tokyo.                                                                                  | Tokyo                                                                                              | []                    |                    2 |               5 | The answer correctly identifies Tokyo as the capital of Japan.                                                                   |\n",
      "+----+----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-----------------------+----------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# get the inputs/outputs details of a finished run.\n",
    "eval_details = pf.get_details(eval_run)\n",
    "# details.head(10)\n",
    "print(tabulate(eval_details.head(), headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics #TODO Not logging locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the metrics of the eval run\n",
    "metrics = pf.get_metrics(eval_run)\n",
    "metrics\n",
    "#print(json.dumps(metrics, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize both the base run and the eval run\n",
    "(Uses local directories: *C:\\Users\\<user name>\\.promptflow*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "The HTML file is generated at 'C:\\\\Users\\\\aprilhazel\\\\AppData\\\\Local\\\\Temp\\\\pf-visualize-detail-smyzdokn.html'.\n",
      "Trying to view the result in a web browser...\n",
      "Successfully visualized from the web browser.\n"
     ]
    }
   ],
   "source": [
    "# # visualize both the base run and the eval run\n",
    "pf.visualize([batch_run, eval_run])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
